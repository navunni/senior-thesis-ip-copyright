{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1263e950-c057-461e-8009-3f66638fd25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGlobal correlation baselines. \\n\\nMy script will reuse the loaders and helpers from `correlation_analysis` to compute: \\n\\n1. For each song pair (PAIR00X):\\n    - Pearson correlation between original and derived for that metric (weekly changes if configured as weekly=True).\\n    - Average Pearson correlation between those two tracks and set of random other tracks in the dataset overlapping time windows. \\n    \\n2. For each event_category:\\n    - Average pair correlation. \\n    - Comparison to global averages (across all the pairs and across all random baselines). \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Global correlation baselines. \n",
    "\n",
    "My script will reuse the loaders and helpers from `correlation_analysis` to compute: \n",
    "\n",
    "1. For each song pair (PAIR00X):\n",
    "    - Pearson correlation between original and derived for that metric (weekly changes if configured as weekly=True).\n",
    "    - Average Pearson correlation between those two tracks and set of random other tracks in the dataset overlapping time windows. \n",
    "    \n",
    "2. For each event_category:\n",
    "    - Average pair correlation. \n",
    "    - Comparison to global averages (across all the pairs and across all random baselines). \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c0c67a-efeb-4a50-b4cb-23c2719464fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, Tuple, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19535e6b-1cb3-41b7-a698-9a3d313bb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8856001-abd7-4255-b32c-f479b57c970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from correlation_analysis import (\n",
    "    load_song_pairs,\n",
    "    load_metric,\n",
    "    weekly_change,\n",
    "    reports_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7090c4f7-4ae6-4473-8391-17a010afdd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This has to match the \"metric\" string used in analyze_pair's rows.\n",
    "metric_label = \"Spotify Streams (Weekly Changes)\"\n",
    "\n",
    "## Monte Carlo bootstrap settings. \n",
    "n_trials = 30 ## Monte Carlo trials/pair\n",
    "n_sample = 30 ## Random tracks/trial\n",
    "n_bootstrap = 400 ## Bootstrap resamples for CI.\n",
    "min_overlap_points = 5 ## minimum weekly overlap for valid correlation\n",
    "random_seed = 42 ## for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e65f56c5-40c4-49f6-bb24-1da40df74b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_corr_dir = reports_dir / \"global_correlation\"\n",
    "global_corr_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98cfd5a0-f5d7-401e-a93b-5ad2215eeb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_jobs = [\n",
    "    {\n",
    "        \"platform\": \"spotify\",\n",
    "        \"metric\": \"streams\",\n",
    "        \"weekly\": True,\n",
    "        \"label\": \"Spotify Streams (Weekly Changes)\",\n",
    "        \"output_prefix\": \"spotify_streams\",\n",
    "    },\n",
    "    {\n",
    "        \"platform\": \"spotify\",\n",
    "        \"metric\": \"popularity\",\n",
    "        \"weekly\": False,  # use levels, not weekly changes\n",
    "        \"label\": \"Spotify Popularity\",\n",
    "        \"output_prefix\": \"spotify_popularity\",\n",
    "    },\n",
    "    {\n",
    "        \"platform\": \"youtube\",\n",
    "        \"metric\": \"views\",\n",
    "        \"weekly\": True,\n",
    "        \"label\": \"YouTube Views (Weekly Changes)\",\n",
    "        \"output_prefix\": \"youtube_views\",\n",
    "    },\n",
    "    {\n",
    "        \"platform\": \"youtube\",\n",
    "        \"metric\": \"likes\",\n",
    "        \"weekly\": True,\n",
    "        \"label\": \"YouTube Likes (Weekly Changes)\",\n",
    "        \"output_prefix\": \"youtube_likes\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26677ad6-d83d-447c-b9c1-04d6d3d02d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrackKey = Tuple[str, str] ## (pair_id, role: either `original` or `derived`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b5dd12-e0fb-4522-b5b0-701af8c31bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(song_pairs: pd.DataFrame, platform: str, metric: str, weekly: bool) -> Dict[TrackKey, Optional[pd.DataFrame]]:\n",
    "    series: Dict[TrackKey, Optional[pd.DataFrame]] = {}\n",
    "    \n",
    "    pair_ids = sorted(song_pairs[\"pair_id\"].astype(str).str.strip().unique())\n",
    "    roles = [\"original\", \"derived\"]\n",
    "    \n",
    "    for pid in pair_ids:\n",
    "        for role in roles:\n",
    "            key: TrackKey = (pid, role)\n",
    "            \n",
    "            ## Loading daily metric:\n",
    "            df_daily = load_metric(pid, role, platform, metric)\n",
    "            \n",
    "            if df_daily is None or df_daily.empty:\n",
    "                series[key] = None\n",
    "                continue\n",
    "                \n",
    "            if not np.issubdtype(df_daily[\"date\"].dtype, np.datetime64):\n",
    "                df_daily = df_daily.copy()\n",
    "                df_daily[\"date\"] = pd.to_datetime(df_daily[\"date\"])\n",
    "            \n",
    "            ## Converting weekly changes:\n",
    "            if weekly:\n",
    "                df_series = weekly_change(df_daily)\n",
    "                # weekly_change is expected to return either None or [\"date\",\"value\"]\n",
    "                if df_series is None or df_series.empty:\n",
    "                    series[key] = None\n",
    "                    continue\n",
    "            else:\n",
    "                df_series = df_daily.copy()\n",
    "                if \"value\" not in df_series.columns:\n",
    "                    # Main heuristic, here, is to pick the first non-date numeric column as a metric.\n",
    "                    value_cols = [c for c in df_series.columns if c != \"date\"]\n",
    "                    num_cols = df_series[value_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "                    chosen = None\n",
    "                    if \"value\" in num_cols:\n",
    "                        chosen = \"value\"\n",
    "                    elif num_cols:\n",
    "                        chosen = num_cols[0]\n",
    "                    elif value_cols:\n",
    "                        chosen = value_cols[0]\n",
    "                    else:\n",
    "                        series[key] = None\n",
    "                        continue\n",
    "\n",
    "                    if chosen != \"value\":\n",
    "                        df_series = df_series.rename(columns={chosen: \"value\"})\n",
    "\n",
    "                # Keepingy only the date and the value.\n",
    "                df_series = df_series[[\"date\", \"value\"]]\n",
    "\n",
    "            series[key] = df_series\n",
    "            \n",
    "    return series     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e529ff5-63c8-4a6d-a95f-99f761f965e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(df_a: Optional[pd.DataFrame], df_b: Optional[pd.DataFrame], min_overlap: int = min_overlap_points,) -> float:\n",
    "    ## Computing Pearson correlation on two weekly series on `date`. \n",
    "    if df_a is None or df_b is None:\n",
    "        return float(\"nan\")\n",
    "    \n",
    "    ## Just to make sure date is in `datetime` format. \n",
    "    for df in (df_a, df_b):\n",
    "        if not np.issubdtype(df[\"date\"].dtype, np.datetime64):\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        \n",
    "    merged = df_a.merge(df_b, on=\"date\", suffixes=(\"_a\", \"_b\")).dropna()\n",
    "    if len(merged) < min_overlap:\n",
    "        return float(\"nan\")\n",
    "    \n",
    "    x = merged[\"value_a\"].to_numpy()\n",
    "    y = merged[\"value_b\"].to_numpy()\n",
    "\n",
    "    # If either series is constant, Pearson is undefined, hence it return NaN without warning.\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return float(\"nan\")\n",
    "    if np.all(x == x[0]) or np.all(y == y[0]):\n",
    "        return float(\"nan\")\n",
    "    \n",
    "    r, _ = pearsonr(x, y)\n",
    "    return float(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2c17e36-2129-4811-b6a4-e0625c85146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(data, n_boot: int = n_bootstrap, ci=95):\n",
    "    \"\"\"\n",
    "    Compute bootstrap confidence interval for the mean of data. \n",
    "    Returns (lower_bound, upper_bound)\n",
    "    \"\"\"\n",
    "    if not data or len(data) == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    \n",
    "    boot_means = []\n",
    "    data = np.array(data)\n",
    "    \n",
    "    for _ in range(n_boot):\n",
    "        sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        boot_means.append(np.mean(sample))\n",
    "    \n",
    "    alpha = (100 - ci) / 100 / 2\n",
    "    lower = np.quantile(boot_means, alpha)\n",
    "    upper = np.quantile(boot_means, 1 - alpha)\n",
    "    \n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09430ae3-5a80-41fd-8998-bca1bd8f3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_random_baseline(series: Dict[TrackKey, Optional[pd.DataFrame]], key_orig: TrackKey, key_deriv: TrackKey, pool: List[TrackKey], n_trials: int = n_trials, n_sample: int = n_sample,) -> tuple[float, List[float]]:\n",
    "    \"\"\"\n",
    "    Monte Carlo simulation of the random baseline. \n",
    "    Repeats random sampling N # of times and averages the resulting mean correlations. \n",
    "    Returns:\n",
    "        - mc_mean (float)\n",
    "        - all_corrs (list of all correlation values across all trials)\n",
    "    \"\"\"\n",
    "    all_corrs:  List[float] = []\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        if not pool: \n",
    "            break\n",
    "            \n",
    "        sample_keys = random.sample(pool, min(n_sample, len(pool)))\n",
    "        \n",
    "        for k in sample_keys:\n",
    "            s_other = series.get(k)\n",
    "            \n",
    "            if s_other is None:\n",
    "                continue\n",
    "                \n",
    "            c1 = corr(series[key_orig], s_other)\n",
    "            if not np.isnan(c1):\n",
    "                all_corrs.append(c1)\n",
    "                \n",
    "            c2 = corr(series[key_deriv], s_other)\n",
    "            if not np.isnan(c2):\n",
    "                all_corrs.append(c2)\n",
    "        \n",
    "    if len(all_corrs) == 0:\n",
    "            return np.nan, []\n",
    "        \n",
    "    return float(np.mean(all_corrs)), all_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50599e6a-f628-436b-b83d-6623e2f28ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_baseline(platform: str, metric: str, weekly: bool, label: str, output_prefix: str,) -> None:\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    ## Loading of metadata and events. \n",
    "    \n",
    "    song_pairs = load_song_pairs()\n",
    "    \n",
    "    print(f\"\\n=== Baseline for {label} ({platform}:{metric}, weekly={weekly}) ===\")\n",
    "    series = compute_all_metrics(song_pairs, platform=platform, metric=metric, weekly=weekly)\n",
    "    \n",
    "    all_track_keys: List[TrackKey] = [\n",
    "        key for key, df in series.items()\n",
    "        if df is not None and len(df) >= min_overlap_points\n",
    "    ]\n",
    "    \n",
    "    if not all_track_keys:\n",
    "        print(\"No usable weekly stream series has been found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total usable track series for my baselines: {len(all_track_keys)}\")\n",
    "    \n",
    "    ## Store pair-level results\n",
    "    pair_level_rows: List[Dict[str, object]] = []\n",
    "    \n",
    "    ## Storing all baseline correlations across all pairs\n",
    "    baseline_corrs: List[float] = []\n",
    "    \n",
    "    unique_pairs = sorted(song_pairs[\"pair_id\"].astype(str).str.strip().unique())\n",
    "    print(f\"Found {len(unique_pairs)} pairs in song_pairs.csv\")\n",
    "    \n",
    "    for pid in unique_pairs:\n",
    "        pid = str(pid).strip()\n",
    "            \n",
    "        ## Obtaining weekly series for original and derived. \n",
    "        \n",
    "        key_orig: TrackKey = (pid, \"original\")\n",
    "        key_deriv: TrackKey = (pid, \"derived\")\n",
    "        s_orig = series.get(key_orig)\n",
    "        s_deriv = series.get(key_deriv)\n",
    "        \n",
    "        if s_orig is None or s_deriv is None:\n",
    "            print(f\"{pid}: Missing weekly streams for either original/derived; Skipping random baseline for this pair\")\n",
    "            continue\n",
    "            \n",
    "        pearson_pair = corr(s_orig, s_deriv)\n",
    "        if np.isnan(pearson_pair):\n",
    "            print(f\"{pid}: Not enough overlap between original and derived; skipping.\")\n",
    "            continue\n",
    "            \n",
    "        ## Building random pool excluding this pair's two tracks. \n",
    "        pool = [k for k in all_track_keys if k not in (key_orig, key_deriv)]\n",
    "        if not pool:\n",
    "            print(f\"{pid}: No remaining tracks for random sampling; skipping\")\n",
    "            continue\n",
    "            \n",
    "        ## Monte Carlo Baseline \n",
    "        \n",
    "        mc_mean, mc_corrs = monte_carlo_random_baseline(series, key_orig, key_deriv, pool, n_trials=50, n_sample=40)\n",
    "        \n",
    "        if len(mc_corrs) == 0:\n",
    "            print(f\"{pid}: No valid Monte Carlo baseline correlations were computed/calculated.\")\n",
    "            continue\n",
    "        \n",
    "        ## Bootstrap CI\n",
    "        \n",
    "        ci_low, ci_high = bootstrap_ci(mc_corrs, n_boot=500, ci=95)\n",
    "        \n",
    "        random_avg = mc_mean\n",
    "        uplift = pearson_pair - random_avg\n",
    "        n_random_corrs = len(mc_corrs)\n",
    "        \n",
    "        ## Extending global baseline correlations (across all pairs)\n",
    "        \n",
    "        baseline_corrs.extend(mc_corrs)\n",
    "\n",
    "        # Obtaining `event_category` from `song_pairs`\n",
    "        meta = song_pairs[song_pairs[\"pair_id\"].astype(str).str.strip() == pid]\n",
    "        event_category = meta[\"event_category\"].iloc[0] if not meta.empty else \"\"\n",
    "\n",
    "        pair_level_rows.append(\n",
    "            {\n",
    "                \"pair_id\": pid,\n",
    "                \"event_category\": event_category,\n",
    "                \"pearson_pair\": pearson_pair,\n",
    "                \"random_avg\": random_avg,\n",
    "                \"random_ci_low\": ci_low,\n",
    "                \"random_ci_high\": ci_high,\n",
    "                \"uplift_vs_random\": uplift,\n",
    "                \"n_random_tracks_sampled\": n_sample,\n",
    "                \"n_random_correlations\": n_random_corrs,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    ## Saving pair-level results to a df. \n",
    "    pair_random_df = pd.DataFrame(pair_level_rows)\n",
    "    out_pairs = global_corr_dir / f\"{output_prefix}_pair_random_baseline.csv\"\n",
    "    pair_random_df.to_csv(out_pairs, index=False)\n",
    "    print(f\"Saved pair-level random baseline results -> {out_pairs}\")\n",
    "\n",
    "    ## Global random baseline average computation.\n",
    "    if baseline_corrs:\n",
    "        global_random_avg = float(np.mean(baseline_corrs))\n",
    "        print(f\"Global random baseline Pearson (all comparisons): {global_random_avg:.3f}\")\n",
    "    else:\n",
    "        global_random_avg = float(\"nan\")\n",
    "        print(\"No global random baseline correlations computed.\")\n",
    "        \n",
    "    ## Event-category summary & global comparison.\n",
    "    \n",
    "    if not pair_random_df.empty:\n",
    "        ## Global mean of pair correlations.\n",
    "        valid_pairs = pair_random_df.dropna(subset=[\"pearson_pair\"])\n",
    "        if not valid_pairs.empty:\n",
    "            global_pair_mean = float(valid_pairs[\"pearson_pair\"].mean())\n",
    "        else:\n",
    "            global_pair_mean = float(\"nan\")\n",
    "            \n",
    "        ## Group-by-event category.\n",
    "        grp = (pair_random_df.groupby(\"event_category\", dropna=False).agg(\n",
    "            n_pairs=(\"pair_id\", \"nunique\"), \n",
    "            avg_pair_corr=(\"pearson_pair\", \"mean\"),\n",
    "            avg_random_corr=(\"random_avg\", \"mean\"),).reset_index())\n",
    "        \n",
    "        ## Comparing category averages to global means. \n",
    "        \n",
    "        grp[\"uplift_vs_global_pair_mean\"] = grp[\"avg_pair_corr\"] - global_pair_mean\n",
    "        grp[\"uplift_vs_global_random_mean\"] = grp[\"avg_pair_corr\"] - global_random_avg\n",
    "        \n",
    "        out_ec = global_corr_dir / f\"{output_prefix}_correlation_summary_spotify_streams.csv\"\n",
    "        grp.to_csv(out_ec, index=False)\n",
    "        print(f\"Saved event-category summary -> {out_ec}\")\n",
    "\n",
    "        # Optional: paired t-test (pair vs random) across all pairs\n",
    "        valid_for_ttest = pair_random_df.dropna(subset=[\"pearson_pair\", \"random_avg\"])\n",
    "        if len(valid_for_ttest) >= 2:\n",
    "            t_res = ttest_rel(valid_for_ttest[\"pearson_pair\"], valid_for_ttest[\"random_avg\"])\n",
    "            print(\n",
    "                f\"\\nPaired t-test (pearson_pair vs random_avg): \"\n",
    "                f\"t = {t_res.statistic:.3f}, p = {t_res.pvalue:.4g}, \"\n",
    "                f\"n = {len(valid_for_ttest)} pairs\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nNot enough pairs with both pearson_pair and random_avg for a paired t-test.\")\n",
    "    else:\n",
    "        print(\"No pair-level rows computed; hence, I am skipping event-category summary.\")\n",
    "    \n",
    "    ## Visualization of Baseline Distribution\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(baseline_corrs, kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of Random Baseline Correlations {label}\")\n",
    "    plt.xlabel(\"Pearson Correlation\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    out_plot = global_corr_dir / f\"{output_prefix}_baseline_distribution.png\"\n",
    "    plt.savefig(out_plot, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved baseline distribution figure -> {out_plot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b99837a-d2fc-43b5-aaac-9e9305a2e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline for Spotify Streams (Weekly Changes) (spotify:streams, weekly=True) ===\n",
      "Total usable track series for my baselines: 50\n",
      "Found 25 pairs in song_pairs.csv\n",
      "PAIR023: Not enough overlap between original and derived; skipping.\n",
      "Saved pair-level random baseline results -> ../reports/visualizations/global_correlation/spotify_streams_pair_random_baseline.csv\n",
      "Global random baseline Pearson (all comparisons): 0.078\n",
      "Saved event-category summary -> ../reports/visualizations/global_correlation/spotify_streams_correlation_summary_spotify_streams.csv\n",
      "\n",
      "Paired t-test (pearson_pair vs random_avg): t = -0.924, p = 0.3651, n = 24 pairs\n",
      "Saved baseline distribution figure -> ../reports/visualizations/global_correlation/spotify_streams_baseline_distribution.png\n",
      "\n",
      "=== Baseline for Spotify Popularity (spotify:popularity, weekly=False) ===\n",
      "Total usable track series for my baselines: 50\n",
      "Found 25 pairs in song_pairs.csv\n",
      "PAIR003: Not enough overlap between original and derived; skipping.\n",
      "Saved pair-level random baseline results -> ../reports/visualizations/global_correlation/spotify_popularity_pair_random_baseline.csv\n",
      "Global random baseline Pearson (all comparisons): 0.192\n",
      "Saved event-category summary -> ../reports/visualizations/global_correlation/spotify_popularity_correlation_summary_spotify_streams.csv\n",
      "\n",
      "Paired t-test (pearson_pair vs random_avg): t = 2.164, p = 0.04109, n = 24 pairs\n",
      "Saved baseline distribution figure -> ../reports/visualizations/global_correlation/spotify_popularity_baseline_distribution.png\n",
      "\n",
      "=== Baseline for YouTube Views (Weekly Changes) (youtube:views, weekly=True) ===\n",
      "Total usable track series for my baselines: 41\n",
      "Found 25 pairs in song_pairs.csv\n",
      "PAIR003: Missing weekly streams for either original/derived; Skipping random baseline for this pair\n",
      "PAIR010: Missing weekly streams for either original/derived; Skipping random baseline for this pair\n",
      "PAIR017: Missing weekly streams for either original/derived; Skipping random baseline for this pair\n",
      "PAIR021: Missing weekly streams for either original/derived; Skipping random baseline for this pair\n",
      "PAIR023: Missing weekly streams for either original/derived; Skipping random baseline for this pair\n",
      "Saved pair-level random baseline results -> ../reports/visualizations/global_correlation/youtube_views_pair_random_baseline.csv\n",
      "Global random baseline Pearson (all comparisons): 0.044\n",
      "Saved event-category summary -> ../reports/visualizations/global_correlation/youtube_views_correlation_summary_spotify_streams.csv\n",
      "\n",
      "Paired t-test (pearson_pair vs random_avg): t = 0.157, p = 0.877, n = 20 pairs\n",
      "Saved baseline distribution figure -> ../reports/visualizations/global_correlation/youtube_views_baseline_distribution.png\n",
      "\n",
      "=== Baseline for YouTube Likes (Weekly Changes) (youtube:likes, weekly=True) ===\n",
      "Total usable track series for my baselines: 40\n",
      "Found 25 pairs in song_pairs.csv\n",
      "PAIR003: Missing weekly streams for either original/derived; Skipping random baseline for this pair\n",
      "PAIR004: Not enough overlap between original and derived; skipping.\n",
      "PAIR006: Missing weekly streams for either original/derived; Skipping random baseline for this pair\n",
      "PAIR010: Missing weekly streams for either original/derived; Skipping random baseline for this pair\n",
      "PAIR017: Missing weekly streams for either original/derived; Skipping random baseline for this pair\n",
      "PAIR020: Not enough overlap between original and derived; skipping.\n",
      "PAIR021: Missing weekly streams for either original/derived; Skipping random baseline for this pair\n",
      "PAIR023: Missing weekly streams for either original/derived; Skipping random baseline for this pair\n",
      "Saved pair-level random baseline results -> ../reports/visualizations/global_correlation/youtube_likes_pair_random_baseline.csv\n",
      "Global random baseline Pearson (all comparisons): 0.076\n",
      "Saved event-category summary -> ../reports/visualizations/global_correlation/youtube_likes_correlation_summary_spotify_streams.csv\n",
      "\n",
      "Paired t-test (pearson_pair vs random_avg): t = 2.741, p = 0.0145, n = 17 pairs\n",
      "Saved baseline distribution figure -> ../reports/visualizations/global_correlation/youtube_likes_baseline_distribution.png\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for job in baseline_jobs:\n",
    "        compute_global_baseline(\n",
    "            platform=job[\"platform\"],\n",
    "            metric=job[\"metric\"],\n",
    "            weekly=job[\"weekly\"],\n",
    "            label=job[\"label\"],\n",
    "            output_prefix=job[\"output_prefix\"],\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
